from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFaceHub  # or other LLM
from langchain_core.documents import Document
from langchain_community.llms import LlamaCpp  # For local models
import requests  # For Ollama API
from langchain_community.llms import HuggingFacePipeline  # For local HuggingFace models
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline
import os

def load_vectorstore():
    # å‡è®¾ä½ ä¹‹å‰å·²ä¿å­˜åˆ° local_db æ–‡ä»¶å¤¹
    return FAISS.load_local(
        folder_path="vector_db",
        embeddings=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"),
        allow_dangerous_deserialization=True  # âœ… æ˜¾å¼å…è®¸ pickle åŠ è½½
    )

class OllamaAPI:
    """Simple wrapper for Ollama API"""
    def __init__(self, model="llama2", temperature=0.2):
        self.model = model
        self.temperature = temperature
        self.api_url = "http://localhost:11434/api/generate"
    
    def __call__(self, prompt):
        response = requests.post(
            self.api_url,
            json={
                "model": self.model,
                "prompt": prompt,
                "temperature": self.temperature
            }
        )
        return response.json()["response"]

def get_local_llm(model_type="huggingface", model_path=None):
    """
    èŽ·å–æœ¬åœ°éƒ¨ç½²çš„LLMæ¨¡åž‹
    
    å‚æ•°:
        model_type: æ¨¡åž‹ç±»åž‹ï¼Œå¯é€‰ "llama", "ollama", "huggingface"
        model_path: æ¨¡åž‹è·¯å¾„ï¼Œå¯¹äºŽæœ¬åœ°æ¨¡åž‹éœ€è¦æŒ‡å®š
    """
    if model_type == "llama":
        # ä½¿ç”¨LlamaCppåŠ è½½æœ¬åœ°æ¨¡åž‹
        return LlamaCpp(
            model_path=model_path,  # ä¾‹å¦‚: "models/deepseek-llm-7b-chat.gguf"
            temperature=0.2,
            max_tokens=512,
            n_ctx=2048,
            verbose=True
        )
    elif model_type == "ollama":
        # ä½¿ç”¨Ollama API
        return OllamaAPI(model="llama2", temperature=0.2)
    elif model_type == "huggingface":
        # ä½¿ç”¨HuggingFaceæœ¬åœ°æ¨¡åž‹
        model_id = model_path or "google/flan-t5-base"  # ä½¿ç”¨æ›´å°çš„æ¨¡åž‹
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        # åˆ›å»ºæ¨¡åž‹ç¼“å­˜ç›®å½•
        cache_dir = os.path.join(os.getcwd(), "model_cache")
        os.makedirs(cache_dir, exist_ok=True)
        
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_id,
            device_map="auto",
            trust_remote_code=True,
            offload_folder=cache_dir  # æŒ‡å®šæƒé‡å¸è½½æ–‡ä»¶å¤¹
        )
        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            temperature=0.2
        )
        return HuggingFacePipeline(pipeline=pipe)
    else:
        # é»˜è®¤ä½¿ç”¨HuggingFaceHub
        return HuggingFaceHub(
            repo_id="google/flan-t5-base",
            model_kwargs={"temperature": 0.2, "max_length": 512}
        )

def ask_question(query: str, model_type="huggingface", model_path=None):
    db = load_vectorstore()

    # èŽ·å–æœ¬åœ°LLM
    llm = get_local_llm(model_type=model_type, model_path=model_path)

    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())
    answer = qa_chain.invoke(query)["result"]  # ä½¿ç”¨invokeæ›¿ä»£run
    return answer

if __name__ == "__main__":
    # è®¾ç½®æ¨¡åž‹ç±»åž‹å’Œè·¯å¾„
    MODEL_TYPE = "huggingface"  # æ”¹ä¸ºhuggingfaceï¼Œå› ä¸ºOllamaéœ€è¦å…ˆæ‹‰å–æ¨¡åž‹
    MODEL_PATH = "google/flan-t5-base"  # ä½¿ç”¨æ›´å°çš„æ¨¡åž‹
    
    print(f"ä½¿ç”¨æ¨¡åž‹ç±»åž‹: {MODEL_TYPE}")
    if MODEL_PATH:
        print(f"æ¨¡åž‹è·¯å¾„: {MODEL_PATH}")
    
    while True:
        question = input("â“ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š\n> ")
        if question.lower() in {"exit", "quit"}:
            break
        answer = ask_question(question, model_type=MODEL_TYPE, model_path=MODEL_PATH)
        print(f"\nðŸ’¬ å›žç­”ï¼š{answer}\n")
